# Local-LLM: DeepSeek-R1-Distill-Qwen-7B 테스트 프로젝트

## 📌 프로젝트 소개
이 프로젝트는 LLM(DeepSeek-R1-Distill-Qwen-7B)을 클라우드 환경에 배포하기 전, 로컬 환경에서 사전 테스트를 진행하고 타당성을 검토하는 것을 목표로 합니다.

### 주요 목표
- MacOS (M3 Pro) 환경에서 DeepSeek 모델을 최적화하여 실행
- MPS (Metal Performance Shaders) 또는 CPU 환경에서 원활한 동작 확인
- 대화 이력을 관리하며 지속적인 대화가 가능하도록 개선
- 모델이 한국어로 자연스럽게 대화할 수 있도록 프롬프트 엔지니어링 적용
- 최적화된 코드 및 환경 설정을 통해 추후 클라우드 배포 가능성을 검토

---

## ⚙️ 프로젝트 스펙
- **모델:** DeepSeek-R1-Distill-Qwen-7B
- **플랫폼:** MacOS (M3 Pro)
- **프레임워크:** PyTorch, Hugging Face `transformers`
- **최적화 방식:** MPS(GPU) 지원 / CPU 사용 최적화
- **대화 관리:** 최근 10개의 대화 기록 유지 (`deque` 활용)
- **언어 지원:** 기본적으로 한국어 대화 유도
- **주요 기능:** 지속적인 대화, 대화 문맥 유지, 사전 테스트를 통한 성능 검토

---

## 🚀 설치 및 실행 방법

### 필수 패키지 설치
아래 명령어를 실행하여 필요한 패키지를 설치합니다.

```bash
pip install torch transformers accelerate sentencepiece numpy scipy
```

---

## 🛠️ 구현 주요 사항

### 1. MPS(GPU) 및 CPU 최적화
	•	M3 Pro의 GPU(MPS)를 사용할 수 있도록 설정하였으며, GPU가 없을 경우 자동으로 CPU를 사용하도록 최적화됨.

### 2. 대화 이력 관리
	•	최근 10개의 대화 이력을 유지하여 문맥이 단절되지 않도록 개선.
	•	AI가 같은 답변을 반복하지 않고, 대화가 더 자연스럽게 연결됨.

### 3. 프롬프트 엔지니어링 적용
	•	초기 프롬프트를 활용하여 AI가 한국어로 대답하도록 유도.

---

## 📊 실행 결과 및 성능 검토

### 1. 성능 테스트 결과
	•	M3 Pro (MPS) 환경에서 실행 가능하며, 응답 속도는 3~5초 내외로 측정됨.
	•	CPU에서 실행할 경우 모델 로딩 시간이 다소 길며, 응답 속도도 느려짐.
	•	한국어 대화 품질과 문맥 유지력이 상대적으로 부족함.

### 2. 문제점 및 개선할 점
	•	모델 로딩 속도가 다소 느림 → 클라우드 배포 시 로드 방식 최적화 필요.
	•	응답 속도가 다소 느림 → 4bit/8bit 양자화 모델 사용 가능성 검토.
	•	한국어 문맥이 어색하고 타 언어와 혼용됨 → 더 큰 모델 (13B 이상) 사용 고려.

---

## 📌 최종 회고

### M3 Pro에서도 LLM 실행은 가능하지만, 학습량에 따라 성능이 제한적이다.
	•	MPS를 활용해도 CPU 대비 극적인 속도 향상은 없었음.
	•	클라우드 환경에서 사용하는 것에는 무리가 있고 OpenAI와 같은 API를 사용하는 것이 아직은 더 나은 것 같음.