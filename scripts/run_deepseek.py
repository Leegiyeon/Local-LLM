import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import deque

# ëª¨ë¸ ê²½ë¡œ
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

# M3 Pro ìµœì í™”: MPS(GPU) ì§€ì› ì—¬ë¶€ í™•ì¸
device = "mps" if torch.backends.mps.is_available() else "cpu"
dtype = torch.bfloat16 if device == "mps" else torch.float32  # MPSì—ì„œëŠ” bfloat16 ì‚¬ìš©

# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=dtype,
    device_map={"": device},
    low_cpu_mem_usage=True
)

# ìµœê·¼ 10ê°œì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë” ë§ì€ ë¬¸ë§¥ ìœ ì§€)
conversation_history = deque(maxlen=10)

# ì´ˆê¸° í”„ë¡¬í”„íŠ¸: ìµœì´ˆ 1íšŒë§Œ ì¶”ê°€
initial_prompt = "AIëŠ” ì˜¤ì§ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
conversation_history.append(initial_prompt)

def generate_response(input_text):
    # ìµœê·¼ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ ëª¨ë¸ ì…ë ¥ ìƒì„±
    history_text = "\n".join(conversation_history) + f"\nì‚¬ìš©ì: {input_text}\nAI:"
    
    inputs = tokenizer(history_text, return_tensors="pt", padding=True, truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # MPS ë˜ëŠ” CPU ì ìš©

    with torch.no_grad():  # ë©”ëª¨ë¦¬ ì ˆì•½
        outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ëŒ€í™” ê¸°ë¡ ì €ì¥ (AI: ë¶€ë¶„ì„ ì œê±°í•´ ë°˜ë³µ ë°©ì§€)
    conversation_history.append(f"ì‚¬ìš©ì: {input_text}")
    conversation_history.append(response)  # AI ë¶€ë¶„ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ

    return response

if __name__ == "__main__":
    print("ğŸ”¹ DeepSeek ëŒ€í™” ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥)")

    while True:
        question = input("\nğŸŸ¢ ì‚¬ìš©ì: ")

        # ì‚¬ìš©ìê°€ 'exit' ë˜ëŠ” 'quit' ì…ë ¥ ì‹œ ì¢…ë£Œ
        if question.lower() in ["exit", "quit"]:
            print("ğŸ”¹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        response = generate_response(question)
        print("\nğŸ¤– AI:", response)